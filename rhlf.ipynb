{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "MODEL LINK : https://huggingface.co/bigcode/tiny_starcoder_py\n",
        "\n",
        "DATASET FOR STEP 2: https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons\n",
        "\n",
        "DATASET FOR STEP 1 and 3: https://huggingface.co/datasets/CarperAI/openai_summarize_tldr"
      ],
      "metadata": {
        "id": "LV4-CQQSsU6S"
      },
      "id": "LV4-CQQSsU6S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa80b0e",
      "metadata": {
        "id": "2fa80b0e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator,\n",
        ")\n",
        "\n",
        "\n",
        "def set_seed(seed_val=42):\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "train_batch_size = 16\n",
        "gradient_accumulation_steps = 1\n",
        "learning_rate = 1e-5\n",
        "eval_batch_size = 1\n",
        "eval_steps = 500\n",
        "max_input_length = 550\n",
        "save_steps = 1000\n",
        "num_train_epochs = 20\n",
        "random.seed(42)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4018c6ca",
      "metadata": {
        "id": "4018c6ca"
      },
      "source": [
        "## Creating the policy model for human Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f90da8c2",
      "metadata": {
        "id": "f90da8c2"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"data/test_policy.parquet\") ## downloded above linked dataset,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00044d2",
      "metadata": {
        "id": "b00044d2",
        "outputId": "da91fb70-a9b4-47cf-d6a9-92f85b2a3ffe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "prompt    SUBREDDIT: r/tifu\\nTITLE: TIFU bY brushing wit...\n",
              "label     Brush Teeth with Baking Soda without research,...\n",
              "Name: 12, dtype: object"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.iloc[12]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0670ff5e",
      "metadata": {
        "id": "0670ff5e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class TLDRDataset(Dataset):\n",
        "    def __init__(self, train_path, tokenizer, split, max_length=256):\n",
        "        self.post_list = []\n",
        "        dataset = pd.read_parquet(train_path)\n",
        "        self.labels = []\n",
        "#         dataset = dataset[:100]\n",
        "        for sample in dataset.iterrows():\n",
        "            self.post_list.append(sample[1][\"prompt\"])\n",
        "            self.labels.append(sample[1][\"label\"])\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.post_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        txt = self.post_list[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encodings_dict = self.tokenizer(txt, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
        "        encodings_dict_label = self.tokenizer(label,truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
        "        input_ids = torch.tensor(encodings_dict[\"input_ids\"])\n",
        "        attn_masks = torch.tensor(encodings_dict[\"attention_mask\"])\n",
        "        labels_ids = torch.tensor(encodings_dict_label[\"input_ids\"])\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attn_masks,\n",
        "            \"labels\": labels_ids,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bced6616",
      "metadata": {
        "id": "bced6616"
      },
      "outputs": [],
      "source": [
        "# for i in TLDRDataset():\n",
        "#     print(i)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f0a73ab",
      "metadata": {
        "id": "7f0a73ab"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigcode/tiny_starcoder_py\", use_cache=False).to(\"cuda:1\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model.config.end_token_id = tokenizer.eos_token_id\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc6aad5",
      "metadata": {
        "id": "dbc6aad5"
      },
      "outputs": [],
      "source": [
        "# Set up the datasets\n",
        "data_path = \"test.parquet\"\n",
        "train_dataset = TLDRDataset(\n",
        "    data_path,\n",
        "    tokenizer,\n",
        "    \"train\",\n",
        "    max_length=256,\n",
        ")\n",
        "# dev_dataset = TLDRDataset(\n",
        "#     data_path,\n",
        "#     tokenizer,\n",
        "#     \"valid\",\n",
        "#     max_length=max_input_length,\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f16a19aa",
      "metadata": {
        "id": "f16a19aa",
        "outputId": "dbf1ac52-40a3-4d6d-b047-5fc7f952aa22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 7100,   613,  2918,   780,    44,   540,    33, 40186,   203, 13777,\n",
            "           44,  3110,   428,    35,    43,   506,    79,   623,  1672, 11970,\n",
            "          428,    35,    43,   488,   614,   646,  3654,   415,   439,  1631,\n",
            "         1159, 16661,  1246,  6366,   973,  3425,    32,   203,  3705,    44,\n",
            "        12000, 17964,  3638,  1548,    32,   439,  9845,   458,  7735,  1330,\n",
            "         5133, 31695,   432,   312,  7000,   372,  7660,   544,  2442,    30,\n",
            "         1273,   439,  4763,  2583, 42289,   312,  3493,   963,   432,  1672,\n",
            "         7713,  1412,   561, 12767,   372,   458, 18734,   308,    59,  4763,\n",
            "         5054,  1755,  1591, 12112,  2670,    30,   461,   436,  5075, 17510,\n",
            "           30,   561,  1597,   963,   432,   322, 48385,   547,   203,   203,\n",
            "         7558,   395,    19,  2770,    30,   312, 17142,   432, 22599, 14818,\n",
            "           30,   439,  7307, 29220,   372,   458,  3932,   107,   544, 18660,\n",
            "           30,  3919,   312,  9525,  2350,   688,   996,  4528,  4335,  1742,\n",
            "          432,    32,   439, 10889,   938,  1597,  3844,   432,   281,  1142,\n",
            "           30,  1259,   439,  4618, 10320,   312,  9396,  2258,   372, 12440,\n",
            "           30,  5774,    30,  5774,    32,  2688,  4484,  4335,  8872, 16512,\n",
            "          821,   322,  2432, 19076,    30,  1259,   439, 39271,   688,   996,\n",
            "         3065, 23840, 18450,   328,  4916,  5049,   996,  4335, 13639,   544,\n",
            "           31, 16033,   352,    32,  2770,    30,   996,  4142,    30,   461,\n",
            "          996,   420,  3280,   963,  8762, 19465,    30,  2258,   619, 22523,\n",
            "           32,   203,   203,  7558,   395,    32,  2770,    30,  8989,   438,\n",
            "         6783, 10191,  4487,    32,  2688, 12440,  2288, 18660,   461, 46438,\n",
            "         6172,    30, 17013,    32,  2770,    30,   358, 30288, 19212,    30,\n",
            "          439,  1597,  2258,   420,  7398,   963,   623,  7024,   461,  6186,\n",
            "          432,  3998,   323, 12837,    30,  1412,   439,  5424,   312, 46438,\n",
            "         6172,   645, 10320, 24240,  2769,   439]) tensor([   59,  6394,  2124,   458,  3932,   107,    30,  1273,  2685,  7696,\n",
            "        27082,   623, 10320,  2685,  1755, 44470, 10320,   436,   312,  5029,\n",
            "        25585,  7918,   432,  1133,    30,   439,  3860, 22958, 14051,   688,\n",
            "          439,  1631,  1159, 17090,  1441,  1672,   663,  3151,   432,   312,\n",
            "        12112,   623, 10320,    32,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0])\n"
          ]
        }
      ],
      "source": [
        "for i in train_dataset:\n",
        "    print(i[\"input_ids\"], i[\"labels\"])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "500301ac",
      "metadata": {
        "id": "500301ac"
      },
      "outputs": [],
      "source": [
        "torch.cuda.set_device(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3801788",
      "metadata": {
        "id": "d3801788"
      },
      "outputs": [],
      "source": [
        "# Prepare the trainer and start training\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=learning_rate,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "#     per_device_eval_batch_size=eval_batch_size,\n",
        "    fp16=False,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    num_train_epochs=2,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baefacaf",
      "metadata": {
        "id": "baefacaf",
        "outputId": "c42dfdf0-63a9-44d1-ac64-53ceff277e65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args.device.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ba4710",
      "metadata": {
        "id": "90ba4710",
        "outputId": "473b9305-5aea-447d-e46f-874f06f82ee9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='410' max='410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [410/410 02:46, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.957300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.951200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.921300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.953300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.955200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.942800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.936000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.943100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.950300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.929100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.930800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.914000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.956000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.952300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.958500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.955300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.938800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.946800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.951500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.940200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.948600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.913400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.954400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.918900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.938500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.934100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.918000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.954000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.961700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.928200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.928800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.932700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.950200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.933300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.958300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.924000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.942000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.954600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=410, training_loss=0.9418552770847227, metrics={'train_runtime': 167.2945, 'train_samples_per_second': 78.341, 'train_steps_per_second': 2.451, 'total_flos': 2417790236491776.0, 'train_loss': 0.9418552770847227, 'epoch': 2.0})"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "#     compute_metrics=compute_metrics,\n",
        "#     data_collator=default_data_collator,\n",
        "#     preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
        ")\n",
        "trainer.train()\n",
        "# trainer.save_model(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dfc50c2",
      "metadata": {
        "id": "3dfc50c2"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"summarization_policy_new/\")   ##path to save policy model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b373c19c",
      "metadata": {
        "id": "b373c19c",
        "outputId": "dfc51c93-f199-4db2-89d1-cc763f31dc2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"summarization_policy_new/\")\n",
        "model_path = \"bigcode/tiny_starcoder_py\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, truncation=True, max_length=256, padding=\"max_length\")\n",
        "text = df.iloc[2][\"prompt\"]\n",
        "tokenized_text = tokenizer(text, return_tensors=\"pt\", max_length=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dc7fe18",
      "metadata": {
        "id": "6dc7fe18",
        "outputId": "e761b8b9-c69a-4459-f6c9-02c3633b84d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Input length of input_ids is 203, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"SUBREDDIT: r/relationships\\nTITLE: The girl [26 F] I [22 M] have been seeing for a month didn't respond to me at all yesterday while hanging out with a friend [~30? M].\\nPOST: She gets terrible service while at her house, but I texted her 3 times yesterday, 4-5 hours apart. She didn't call me until early this morning and left a voicemail that she was busy all day with a friend who showed up out of the blue.\\n\\nI saw that she posted a picture of the two of them out of her dead zone house on facebook before I texted her the last time.\\n\\nI don't mind that she hangs out with friends, and I know it's pretty early in the relationship, but am I wrong to be a little annoyed that she didn't respond until 24 hours after my first text?\\nTL;DR: <|endoftext|>\""
            ]
          },
          "execution_count": 185,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(model.generate(**tokenized_text)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8421708e",
      "metadata": {
        "id": "8421708e"
      },
      "source": [
        "## Traning the reward function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b607ce4",
      "metadata": {
        "id": "5b607ce4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
        "from trl import RewardTrainer, SFTTrainer\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import Trainer, TrainingArguments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e849ca0f",
      "metadata": {
        "id": "e849ca0f"
      },
      "outputs": [],
      "source": [
        "##model path\n",
        "MODEL_PATH = \"bigcode/tiny_starcoder_py\"\n",
        "DATA_PATH = \"data/test.parquet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14ddb7e5",
      "metadata": {
        "id": "14ddb7e5",
        "outputId": "8001d1b3-25fc-4542-a827-bc00e6f1bc93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'chosen', 'rejected'],\n",
              "    num_rows: 10\n",
              "})"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_parquet(DATA_PATH)\n",
        "df = df[:10]\n",
        "raw_dataset = Dataset.from_pandas(df)\n",
        "raw_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2209d915",
      "metadata": {
        "id": "2209d915"
      },
      "outputs": [],
      "source": [
        "##defininig the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "368defaa",
      "metadata": {
        "id": "368defaa"
      },
      "outputs": [],
      "source": [
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "def formatting_func(examples):\n",
        "    kwargs = {\"padding\": \"max_length\",\n",
        "              \"truncation\": True,\n",
        "              \"max_length\": 256,\n",
        "              \"return_tensors\": \"pt\"\n",
        "              }\n",
        "\n",
        "    # Prepend the prompt and a line break to the original_response and response-1 fields.\n",
        "    prompt_plus_chosen_response = examples[\"prompt\"] + \"\\n\" + examples[\"chosen\"]\n",
        "    prompt_plus_rejected_response = examples[\"prompt\"] + \"\\n\" + examples[\"rejected\"]\n",
        "\n",
        "    # Then tokenize these modified fields.\n",
        "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
        "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
        "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4afbee",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "0f4afbee",
        "outputId": "2cc1a31c-16b7-47cd-e5ae-e6e160386fa4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "formatted_dataset = raw_dataset.map(formatting_func)\n",
        "formatted_dataset = formatted_dataset.train_test_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43490bac",
      "metadata": {
        "id": "43490bac",
        "outputId": "f3d87057-0f54-478c-9e91-23ad6b61741e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTBigCodeConfig {\n",
              "  \"_name_or_path\": \"model/\",\n",
              "  \"activation_function\": \"gelu_pytorch_tanh\",\n",
              "  \"architectures\": [\n",
              "    \"GPTBigCodeForCausalLM\"\n",
              "  ],\n",
              "  \"attention_softmax_in_fp32\": true,\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 0,\n",
              "  \"inference_runner\": 0,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"max_batch_size\": null,\n",
              "  \"max_sequence_length\": null,\n",
              "  \"model_type\": \"gpt_bigcode\",\n",
              "  \"multi_query\": true,\n",
              "  \"n_embd\": 768,\n",
              "  \"n_head\": 12,\n",
              "  \"n_inner\": 3072,\n",
              "  \"n_layer\": 20,\n",
              "  \"n_positions\": 8192,\n",
              "  \"pad_key_length\": true,\n",
              "  \"pre_allocate_kv_cache\": false,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attention_softmax_in_fp32\": true,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.29.0\",\n",
              "  \"use_cache\": true,\n",
              "  \"validate_runner_input\": true,\n",
              "  \"vocab_size\": 49152\n",
              "}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce3aef6",
      "metadata": {
        "id": "dce3aef6"
      },
      "outputs": [],
      "source": [
        "### Loading the TRL reward trainer and training the trainer\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=\"rm_checkpoint/\",\n",
        "        num_train_epochs=1,\n",
        "        logging_steps=10,\n",
        "        gradient_accumulation_steps=1,\n",
        "        save_strategy=\"steps\",\n",
        "        evaluation_strategy=\"steps\",\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=1,\n",
        "        eval_accumulation_steps=1,\n",
        "        eval_steps=500,\n",
        "        save_steps=500,\n",
        "        warmup_steps=100,\n",
        "        logging_dir=\"./logs\",\n",
        "        learning_rate=1e-5,\n",
        "        save_total_limit=1,\n",
        "        no_cuda=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d06ad45a",
      "metadata": {
        "id": "d06ad45a",
        "outputId": "ec58bb3d-6bae-41d7-8485-a6cd24ac6eaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\nri\\rhlf-trainer\\env\\Lib\\site-packages\\trl\\trainer\\reward_trainer.py:128: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in the RewardTrainer's init it will be set to `512` by default, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "C:\\nri\\rhlf-trainer\\env\\Lib\\site-packages\\trl\\trainer\\reward_trainer.py:139: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "C:\\nri\\rhlf-trainer\\env\\Lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "C:\\nri\\rhlf-trainer\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2382: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:17, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4, training_loss=0.7686203718185425, metrics={'train_runtime': 24.8182, 'train_samples_per_second': 0.282, 'train_steps_per_second': 0.161, 'total_flos': 0.0, 'train_loss': 0.7686203718185425, 'epoch': 1.0})"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = RewardTrainer(model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        train_dataset=formatted_dataset['train'],\n",
        "                        eval_dataset=formatted_dataset['test'],\n",
        "                        args= training_args\n",
        "                        )\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8620e81",
      "metadata": {
        "id": "c8620e81"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"rm_model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d56a13c",
      "metadata": {
        "id": "1d56a13c"
      },
      "outputs": [],
      "source": [
        "## inference the model\n",
        "rm_model = AutoModelForCausalLM.from_pretrained(\"rm_model/\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e694fbc6",
      "metadata": {
        "id": "e694fbc6"
      },
      "outputs": [],
      "source": [
        "def get_score(model, tokenizer, prompt, response):\n",
        "\n",
        "    instructions = tokenizer.encode_plus(prompt,\n",
        "                                       response,\n",
        "                                       padding=\"max_length\",\n",
        "                                       max_length=256,\n",
        "                                       return_tensors=\"pt\",\n",
        "                                        truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**instructions)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed481305",
      "metadata": {
        "id": "ed481305"
      },
      "outputs": [],
      "source": [
        "# usage with prompt\n",
        "prompt = df.iloc[0][\"prompt\"]\n",
        "example_prefered_response = df.iloc[0][\"chosen\"]\n",
        "example_unprefered_response = df.iloc[0][\"rejected\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d513768",
      "metadata": {
        "id": "1d513768"
      },
      "outputs": [],
      "source": [
        "loss1 = get_score(model, tokenizer, prompt, example_prefered_response)\n",
        "loss2= get_score(model, tokenizer, prompt, example_unprefered_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc95e92f",
      "metadata": {
        "id": "bc95e92f"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "loss = -nn.functional.logsigmoid(loss1 - loss2).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c5e14b4",
      "metadata": {
        "id": "1c5e14b4",
        "outputId": "9e19b9f4-8eae-4174-cfc3-1be26519d101"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'__IT_\\n       \"r/\\n: r RelationshipRelationship]]0]\\nlsriend\\n2//M]\\n [ [ a\\n the was to the [. a friends to\\n\\n:\\n [lfriend [ me have a aried in his19 minutes.\\n\\n\"\"\"What Modified:** girlfriend was through the Facebook.. I my my friends.**** my  of lf**\\n\\n** was d1ing for my few personirl** I had for findoolpping my my the future** but I was that in\\n\\n** have ali  of to she  tolirt my me girl. and she found my about my.. me few of gir.1viously). was\\'t find her was).\\n\\n** was it about my twoirl and the had  Facebook. the  and she gand historyirl) was in April,\\n to, find, were flirted. I a messages.. f.ing on her.\\n girlM\\n; I1 girirllfriend and the19 months. to my Facebook.. my permission. she her messages. my.lirty with my fewirl.\\n found her with me. I through more with\\n'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(torch.max(loss1, axis=-1).indices[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92769534",
      "metadata": {
        "id": "92769534"
      },
      "source": [
        "# Policy Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9cb217",
      "metadata": {
        "id": "bb9cb217"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling\n",
        "from trl import RewardTrainer, SFTTrainer\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50d0acdb",
      "metadata": {
        "id": "50d0acdb"
      },
      "outputs": [],
      "source": [
        "##model path\n",
        "MODEL_PATH = \"rm_model/\"\n",
        "DATA_PATH = \"data/test.parquet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "221cfe68",
      "metadata": {
        "id": "221cfe68",
        "outputId": "26ab818a-ffcf-4594-82d5-0073c32c66aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'chosen', 'rejected'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_parquet(DATA_PATH)\n",
        "df = df[:1000]\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f02097",
      "metadata": {
        "id": "05f02097"
      },
      "outputs": [],
      "source": [
        "sentiment_pipe_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\"}\n",
        "\n",
        "config = PPOConfig(\n",
        "    model_name=MODEL_PATH, steps=51200, learning_rate=1.41e-5, remove_unused_columns=True\n",
        ")\n",
        "\n",
        "txt_in_len = 5\n",
        "txt_out_len = 20\n",
        "seed = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e5769b3",
      "metadata": {
        "id": "6e5769b3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40d0ef6a",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "40d0ef6a",
        "outputId": "a3f689c0-c3e9-47c3-c74e-0f9a5fae662f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = dataset.rename_columns({\"prompt\": \"review\"})\n",
        "dataset = dataset.filter(lambda x: len(x[\"review\"]) > 500, batched=False)\n",
        "dataset = dataset.map(lambda x: {\"review\": x[\"review\"][:1000]}, batched=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d7e8b8",
      "metadata": {
        "id": "96d7e8b8"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3adda4dc",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "3adda4dc",
        "outputId": "c10d2a3c-9143-4e2c-8953-7cc286e5b9c8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "txt_in_len = 5\n",
        "txt_out_len = 32\n",
        "seed = 1\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda x: {\"input_ids\": tokenizer.encode(\" \" + x[\"chosen\"], return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=32)[0]},\n",
        "    batched=False,\n",
        ")\n",
        "dataset = dataset.map(lambda x: {\"query\": tokenizer.decode(x[\"input_ids\"])}, batched=False)\n",
        "dataset = dataset[:20480]\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_dict(dataset)\n",
        "dataset.set_format(\"pytorch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e189a22b",
      "metadata": {
        "id": "e189a22b"
      },
      "outputs": [],
      "source": [
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc612f5f",
      "metadata": {
        "id": "cc612f5f"
      },
      "outputs": [],
      "source": [
        "rf_model_path = \"rm_model/\"\n",
        "starcoder_model = AutoModelForCausalLMWithValueHead.from_pretrained(\"summarization_policy_new/\")  ##policy model from step 1\n",
        "starcoder_model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(rf_model_path) ## reward model from step 2\n",
        "starcoder_tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\") ## tokenizer of step 1 model., here since we are using same model for step 1 and 2 it doesnot matter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2836e5cc",
      "metadata": {
        "id": "2836e5cc",
        "outputId": "f8ea4bec-594c-4c55-8952-56b6731a90fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['review', 'chosen', 'rejected', 'input_ids', 'query'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4800d3e9",
      "metadata": {
        "id": "4800d3e9"
      },
      "outputs": [],
      "source": [
        "# starcoder_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e1a2e38",
      "metadata": {
        "id": "2e1a2e38"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "optimizer = torch.optim.SGD(starcoder_model.parameters(), lr=config.learning_rate)\n",
        "ppo_trainer = PPOTrainer(config, starcoder_model, starcoder_model, starcoder_tokenizer, dataset=dataset, data_collator=collator, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e63e72ad",
      "metadata": {
        "id": "e63e72ad"
      },
      "outputs": [],
      "source": [
        "# for i in ppo_trainer.dataloader:\n",
        "#   print(i)\n",
        "#   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e329e292",
      "metadata": {
        "id": "e329e292"
      },
      "outputs": [],
      "source": [
        "ctrl_str = [\"[negative]\", \"[positive]\"]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # this should be handled by accelerate\n",
        "ctrl_tokens = dict((s, starcoder_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "597ac197",
      "metadata": {
        "id": "597ac197"
      },
      "outputs": [],
      "source": [
        "def pos_logit_to_reward(logit, task):\n",
        "    \"\"\"\n",
        "    Take the positive sentiment logit and scale it for the task.\n",
        "        task [negative]: reward = -logit\n",
        "        task [neutral]: reward = -2*abs(logit)+4\n",
        "        task [positive]: reward = logit\n",
        "    \"\"\"\n",
        "    for i in range(len(logit)):\n",
        "        if task[i] == \"[negative]\":\n",
        "            logit[i] = -logit[i]\n",
        "        elif task[i] == \"[positive]\":\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(\"task has to be in [0, 1, 2]!\")\n",
        "    return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a916542",
      "metadata": {
        "id": "8a916542",
        "outputId": "b3fbbee8-1fa3-498f-fe7e-15c2d39702b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([-4.,  4.])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_logit_to_reward(torch.Tensor([4, 4]), ctrl_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2974cb44",
      "metadata": {
        "id": "2974cb44"
      },
      "outputs": [],
      "source": [
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": starcoder_tokenizer.eos_token_id,\n",
        "    \"max_new_tokens\": 32,\n",
        "    \"eos_token_id\": -1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27cf0762",
      "metadata": {
        "id": "27cf0762"
      },
      "outputs": [],
      "source": [
        "def get_score(model, tokenizer, responses):\n",
        "    positive_logist = []\n",
        "    for i in responses:\n",
        "        instructions = tokenizer.encode_plus(\n",
        "                                           i,\n",
        "                                           padding=\"max_length\",\n",
        "                                           max_length=32,\n",
        "                                           return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**instructions)\n",
        "\n",
        "        logits = outputs[0].mean()\n",
        "        positive_logist.append(logits)\n",
        "\n",
        "    return positive_logist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d47d37",
      "metadata": {
        "id": "41d47d37"
      },
      "outputs": [],
      "source": [
        "# responses =[\"ashish is a goo\", \"heelow how are you\", \"__IT_\\nr/\\n: r RelationshipRelationship]]0]\\nlsriend\\n2//M]\\n [ [ a\\n the was to the [. a friends to\\n\\n:\\n [lfriend [ me have a aried in his19 minutes.\\n\\nWhat Modified:** girlfriend was through the Facebook.. I my my friends.**** my  of lf**\\n\\n** was d1ing for my few personirl** I had for findoolpping my my the future** but I was that in\\n\\n** have ali  of to she  tolirt my me girl. and she found my about my.. me few of gir.1viously). was\\'t find her was).\\n\\n** was it about my twoirl and the had  Facebook. the  and she gand historyirl) was in April,\\n to, find, were flirted. I a messages.. f.ing on her.\\n girlM\\n; I1 girirllfriend and the19 months. to my Facebook.. my permission. she her messages. my.lirty with my fewirl.\\n found her with me. I through more with\\n\"]\n",
        "# get_score(starcoder_model, tokenizer, responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9e99bd",
      "metadata": {
        "id": "8b9e99bd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "253d6cc8",
      "metadata": {
        "id": "253d6cc8",
        "outputId": "84898817-5381-4656-d0de-75c7dc80803d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            "  0%|                                                                                                                        | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[negative]', '[positive]']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            " 33%|                                                                         | 1/3 [25:43<51:27, 1543.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[negative]', '[positive]']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r",
            " 67%|                                    | 2/3 [49:32<24:36, 1476.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[negative]', '[positive]']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 3/3 [1:13:37<00:00, 1472.59s/it]\n"
          ]
        }
      ],
      "source": [
        "from random import choices\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "for epoch in range(1):\n",
        "    for batch in tqdm(ppo_trainer.dataloader):\n",
        "        (logs, game_data,) = (\n",
        "            dict(),\n",
        "            dict(),\n",
        "        )\n",
        "\n",
        "        print(ctrl_str)\n",
        "        #### prepend a random control token\n",
        "        task_list = choices(ctrl_str, k=config.batch_size)\n",
        "        game_data[\"query\"] = [t + q for t, q in zip(task_list, batch[\"query\"])]\n",
        "        query_tensors = [torch.cat((ctrl_tokens[t], input_ids)) for t, input_ids in zip(task_list, batch[\"input_ids\"])]\n",
        "\n",
        "        #### get response from gpt2\n",
        "        response_tensors = []\n",
        "        for query in query_tensors:\n",
        "            response = ppo_trainer.generate(query, **generation_kwargs)\n",
        "            response_tensors.append(response.squeeze()[-txt_out_len:])\n",
        "#         print(response_tensors)\n",
        "        game_data[\"response\"] = [starcoder_tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "\n",
        "        #### sentiment analysis\n",
        "        texts = [q + r for q, r in zip(batch[\"query\"], game_data[\"response\"])]\n",
        "        logits = get_score(starcoder_model,starcoder_tokenizer, texts)\n",
        "        rewards = pos_logit_to_reward(logits, task_list)\n",
        "\n",
        "        #### Run PPO training\n",
        "        t = time.time()\n",
        "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "\n",
        "        for cs in ctrl_str:\n",
        "            key = \"env/reward_\" + cs.strip(\"[]\")\n",
        "            stats[key] = np.mean([r.cpu().numpy() for r, t in zip(rewards, task_list) if t == cs])\n",
        "        ppo_trainer.log_stats(stats, game_data, rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0bad48f",
      "metadata": {
        "id": "d0bad48f",
        "outputId": "dba21dc2-3836-4df1-a87c-f201eaf126fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('rhlfmodel/tokenizer_config.json',\n",
              " 'rhlfmodel/special_tokens_map.json',\n",
              " 'rhlfmodel/vocab.json',\n",
              " 'rhlfmodel/merges.txt',\n",
              " 'rhlfmodel/added_tokens.json',\n",
              " 'rhlfmodel/tokenizer.json')"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "###saving the model\n",
        "starcoder_model.save_pretrained(\"rhlfmodel/\")\n",
        "starcoder_tokenizer.save_pretrained(\"rhlfmodel/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23bf2489",
      "metadata": {
        "id": "23bf2489",
        "outputId": "314e7241-83ab-4e3e-ccb2-79fe49813dd4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at rhlfmodel/ were not used when initializing GPTBigCodeForCausalLM: ['v_head.summary.bias', 'v_head.summary.weight']\n",
            "- This IS expected if you are initializing GPTBigCodeForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPTBigCodeForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "model_path = \"rhlfmodel/\"\n",
        "set_seed(42)\n",
        "pipe = pipeline(\"text-generation\",model=model_path, tokenizer=model_path, max_length=30, num_return_sequences=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "026e2c6c",
      "metadata": {
        "id": "026e2c6c"
      },
      "outputs": [],
      "source": [
        "# text = dataset[\"rejected\"][0]\n",
        "# pipe(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85b6a2e4",
      "metadata": {
        "id": "85b6a2e4"
      },
      "outputs": [],
      "source": [
        "# text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e8d8135",
      "metadata": {
        "scrolled": false,
        "id": "8e8d8135",
        "outputId": "c7f23370-09e6-4305-eee5-32428a000dd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'objective/kl': 0.0,\n",
              " 'objective/kl_dist': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0.], dtype=float32),\n",
              " 'objective/logprobs': array([[-2.4248543e+01, -1.4291405e+01, -2.6734692e+01, ...,\n",
              "         -3.1947580e-05, -3.2424403e-05, -3.5285328e-05],\n",
              "        [-2.5522242e+01, -1.4627422e+01, -2.7209345e+01, ...,\n",
              "         -2.9682673e-05, -3.0278701e-05, -3.2901222e-05],\n",
              "        [-2.4248543e+01, -1.4291405e+01, -2.6734692e+01, ...,\n",
              "         -3.3735661e-05, -3.4450892e-05, -3.7550220e-05],\n",
              "        ...,\n",
              "        [-2.4248543e+01, -1.4291405e+01, -9.7971582e+00, ...,\n",
              "         -3.0874729e-05, -3.1232346e-05, -3.3735661e-05],\n",
              "        [-2.5522242e+01, -1.4627422e+01, -2.7209345e+01, ...,\n",
              "         -2.8729026e-05, -2.9325056e-05, -3.1828375e-05],\n",
              "        [-2.4248543e+01, -1.4291405e+01, -9.7971582e+00, ...,\n",
              "         -3.1470758e-05, -3.1828375e-05, -3.4450892e-05]], dtype=float32),\n",
              " 'objective/ref_logprobs': array([[-2.4248543e+01, -1.4291405e+01, -2.6734692e+01, ...,\n",
              "         -3.1947580e-05, -3.2424403e-05, -3.5285328e-05],\n",
              "        [-2.5522242e+01, -1.4627422e+01, -2.7209345e+01, ...,\n",
              "         -2.9682673e-05, -3.0278701e-05, -3.2901222e-05],\n",
              "        [-2.4248543e+01, -1.4291405e+01, -2.6734692e+01, ...,\n",
              "         -3.3735661e-05, -3.4450892e-05, -3.7550220e-05],\n",
              "        ...,\n",
              "        [-2.4248543e+01, -1.4291405e+01, -9.7971582e+00, ...,\n",
              "         -3.0874729e-05, -3.1232346e-05, -3.3735661e-05],\n",
              "        [-2.5522242e+01, -1.4627422e+01, -2.7209345e+01, ...,\n",
              "         -2.8729026e-05, -2.9325056e-05, -3.1828375e-05],\n",
              "        [-2.4248543e+01, -1.4291405e+01, -9.7971582e+00, ...,\n",
              "         -3.1470758e-05, -3.1828375e-05, -3.4450892e-05]], dtype=float32),\n",
              " 'objective/kl_coef': 0.19694370179645443,\n",
              " 'objective/entropy': 1.6749732494354248,\n",
              " 'ppo/mean_non_score_reward': 0.0,\n",
              " 'ppo/mean_scores': -0.12584742903709412,\n",
              " 'ppo/std_scores': 5.42613410949707,\n",
              " 'tokens/queries_len_mean': 35.0,\n",
              " 'tokens/queries_len_std': 0.0,\n",
              " 'tokens/queries_dist': array([35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35., 35.,\n",
              "        35., 35., 35., 35., 35., 35., 35., 35., 35.], dtype=float32),\n",
              " 'tokens/responses_len_mean': 32.0,\n",
              " 'tokens/responses_len_std': 0.0,\n",
              " 'tokens/responses_dist': array([32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32., 32.,\n",
              "        32., 32., 32., 32., 32., 32., 32., 32., 32.], dtype=float32),\n",
              " 'ppo/loss/policy': -0.00010979006765410304,\n",
              " 'ppo/loss/value': 5.189159870147705,\n",
              " 'ppo/loss/total': 0.5188061594963074,\n",
              " 'ppo/policy/entropy': 0.0009501720196567476,\n",
              " 'ppo/policy/approxkl': 2.324135675735306e-05,\n",
              " 'ppo/policy/policykl': 2.1984804334351793e-05,\n",
              " 'ppo/policy/clipfrac': 0.000244140625,\n",
              " 'ppo/policy/advantages': array([1.8667955, 1.8631277, 1.8592668, ..., 0.8031267, 2.2571814,\n",
              "        1.3958192], dtype=float32),\n",
              " 'ppo/policy/advantages_mean': 4.2527972254902124e-09,\n",
              " 'ppo/policy/ratio': array([1.        , 1.        , 1.        , ..., 0.9999918 , 0.99999154,\n",
              "        0.99999094], dtype=float32),\n",
              " 'ppo/returns/mean': 0.21962669491767883,\n",
              " 'ppo/returns/var': 1.6779695749282837,\n",
              " 'ppo/val/vpred': 0.4521799087524414,\n",
              " 'ppo/val/error': 9.344433784484863,\n",
              " 'ppo/val/clipfrac': 0.394866943359375,\n",
              " 'ppo/val/mean': 0.5472686290740967,\n",
              " 'ppo/val/var': 0.22415316104888916,\n",
              " 'ppo/time/ppo/optimizer_step': 0.12164130806922913,\n",
              " 'ppo/val/var_explained': -4.5688934326171875,\n",
              " 'ppo/learning_rate': 1.41e-05,\n",
              " 'time/ppo/forward_pass': 84.3550615310669,\n",
              " 'time/ppo/compute_rewards': 0.015000104904174805,\n",
              " 'time/ppo/optimize_step': 1003.6978762149811,\n",
              " 'time/ppo/calc_stats': 0.17599773406982422,\n",
              " 'time/ppo/total': 1088.264935016632,\n",
              " 'env/reward_negative': 5.415969,\n",
              " 'env/reward_positive': -5.41384}"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f478497",
      "metadata": {
        "id": "2f478497"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}